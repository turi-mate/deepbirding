{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":44224,"databundleVersionId":5188730,"sourceType":"competition"},{"sourceId":6912431,"sourceType":"datasetVersion","datasetId":3969818},{"sourceId":7137902,"sourceType":"datasetVersion","datasetId":4119162}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install audiomentations -q","metadata":{"ExecuteTime":{"end_time":"2023-12-10T09:36:28.803574700Z","start_time":"2023-12-10T09:36:17.228464200Z"},"execution":{"iopub.status.busy":"2023-12-10T10:34:10.407044Z","iopub.execute_input":"2023-12-10T10:34:10.407444Z","iopub.status.idle":"2023-12-10T10:34:26.744020Z","shell.execute_reply.started":"2023-12-10T10:34:10.407412Z","shell.execute_reply":"2023-12-10T10:34:26.742336Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"#### Importing essential packages for the training","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport librosa\nimport random\nimport wandb\nimport torch.utils\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport os\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nimport pickle\nfrom audiomentations import Compose, AddGaussianNoise, Gain, AddBackgroundNoise, Shift, AdjustDuration, Normalize\nimport torchvision","metadata":{"ExecuteTime":{"end_time":"2023-12-10T09:36:45.878594500Z","start_time":"2023-12-10T09:36:41.714091600Z"},"execution":{"iopub.status.busy":"2023-12-10T10:34:26.746594Z","iopub.execute_input":"2023-12-10T10:34:26.747037Z","iopub.status.idle":"2023-12-10T10:34:45.618915Z","shell.execute_reply.started":"2023-12-10T10:34:26.746990Z","shell.execute_reply":"2023-12-10T10:34:45.617685Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Data Loading","metadata":{}},{"cell_type":"markdown","source":"#### Defining some functions for loading the dataset, such as normalizing the Mel Spectogram, extracting the metadata, for the imbalanced dataset we are using class weighting, and we are also endocing our labels\n ","metadata":{}},{"cell_type":"code","source":"def normalize_mel_spectrogram(mel_spec_tensor):\n    min_value = mel_spec_tensor.min()\n    max_value = mel_spec_tensor.max()\n    normalized_mel_spec = (mel_spec_tensor - min_value) / (max_value - min_value)\n    return normalized_mel_spec\n\ndef extract_metadata(metadata_file):\n    metadata_df = pd.read_csv(metadata_file)\n    metadata_df['Filename'] = metadata_df['Filename'].apply(lambda x: x.split(\"/\")[-1])\n    metadata = metadata_df[['Label', 'Filename']].reset_index(drop=True)\n    metadata.columns = ['label', 'filename']\n    return metadata\n\ndef class_weights(labels):\n    class_weights = (labels.value_counts() / labels.value_counts().sum()) ** (-0.5)\n    normalized_weights = class_weights / class_weights.sum()\n    normalized_weights = normalized_weights*80\n    return torch.tensor(normalized_weights.values, dtype=torch.float16)\n\ndef encode_labels(labels):\n    unique_labels = labels.unique()\n    encoded_dict = {label: num for label, num in zip(unique_labels, range(len(unique_labels)))}\n    return encoded_dict","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining Our Custom Audio Classification Dataset","metadata":{}},{"cell_type":"markdown","source":"#### We are applying augmentations to the waveform, such as adding Gaussian noise, adding gain, adding time shifting, and adding background raining noise with different probabilites","metadata":{}},{"cell_type":"code","source":"class AudioClassificationDataset(Dataset):\n    def __init__(self, data_dir, metadata_file, apply_augmentations=False):\n        self.data_dir = data_dir\n        self.metadata = extract_metadata(metadata_file)\n        self.apply_augmentations = apply_augmentations\n        self.label_encoder = encode_labels(self.metadata['label']) \n        self.class_weights = class_weights(self.metadata['label'])\n        self.augmentations = Compose([\n            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.4),\n            Gain(min_gain_in_db=-12, max_gain_in_db=12, p=0.4),    \n            Shift(min_shift=1, max_shift=6, shift_unit=\"seconds\", p=0.6),\n            AddBackgroundNoise(sounds_path=[\"/kaggle/input/backg-noise/rain-noise.wav\"],max_absolute_rms_db=-30, p=0.3),\n        ])\n\n    def __len__(self):\n        return len(self.metadata)\n\n    def __getitem__(self, idx):\n        label = self.metadata['label'][idx]\n        filename = self.metadata['filename'][idx]\n        audio_file = os.path.join(self.data_dir, label, filename) \n        try:\n            waveform, sample_rate = librosa.load(audio_file, sr=None)\n        except Exception as e:\n            print(f\"Error loading audio file {audio_file}: {e}\")\n            return None   \n        if self.apply_augmentations:\n            waveform = self.augmentations(waveform, sample_rate=sample_rate)\n        normalization_transform = Normalize(p=1.0)\n        duration_transform = AdjustDuration(duration_seconds=10,padding_mode=\"wrap\", p=1.0)\n        waveform = normalization_transform(waveform, sample_rate=sample_rate)\n        waveform = duration_transform(waveform, sample_rate=sample_rate)\n        mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate, fmin=800, fmax=12000)\n        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n        mel_spectrogram_db = normalize_mel_spectrogram(mel_spectrogram_db)\n        mel_spec_db_tensor = torch.from_numpy(mel_spectrogram_db).unsqueeze(0)\n        class_label = self.label_encoder[label]\n        class_label_tensor = torch.tensor(class_label).unsqueeze(0)\n        return mel_spec_db_tensor, class_label_tensor","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.517400Z","iopub.execute_input":"2023-12-09T17:49:25.517721Z","iopub.status.idle":"2023-12-09T17:49:25.538548Z","shell.execute_reply.started":"2023-12-09T17:49:25.517693Z","shell.execute_reply":"2023-12-09T17:49:25.537562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating two different datasets, one with augmentations applied for the training, one without augmentations for the validation part","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/birdclef-2023/train_audio'\nmetadata_file = '/kaggle/input/filtered-metadata/filtered_metadata.csv'\ntrain_dataset = AudioClassificationDataset(data_dir, metadata_file, apply_augmentations=True)\nval_dataset = AudioClassificationDataset(data_dir, metadata_file, apply_augmentations=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.539741Z","iopub.execute_input":"2023-12-09T17:49:25.540483Z","iopub.status.idle":"2023-12-09T17:49:25.640587Z","shell.execute_reply.started":"2023-12-09T17:49:25.540456Z","shell.execute_reply":"2023-12-09T17:49:25.639781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We split the original dataset and chose random indicies with 70% training 20% validation and 10% test parts","metadata":{}},{"cell_type":"code","source":"train_ratio = 0.7\nval_ratio = 0.2\ntest_ratio = 0.1  \n\nnum_samples = len(train_dataset)\nnum_train = int(train_ratio * num_samples)\nnum_val = int(val_ratio * num_samples)\nnum_test = int(test_ratio * num_samples)\nprint(f'Training indices: {num_train}  Validation indices: {num_val} Test indicies: {num_test}')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We are making sure that the test indicies are selected randomly but most importantly deterministically with 42 seed, so we can have uniform indicies for testing","metadata":{}},{"cell_type":"code","source":"indices = list(range(num_samples))\nrandom.seed(42)\nrandom.shuffle(indices)\n\ntest_indices = indices[-num_test:]\nwith open('test_indices.pkl', 'wb') as f:\n    pickle.dump(test_indices, f)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.641993Z","iopub.execute_input":"2023-12-09T17:49:25.642358Z","iopub.status.idle":"2023-12-09T17:49:25.675510Z","shell.execute_reply.started":"2023-12-09T17:49:25.642323Z","shell.execute_reply":"2023-12-09T17:49:25.674560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The remaining indicies can be chosen for the training and validation indicies ","metadata":{}},{"cell_type":"code","source":"remaining_indices = list(set(indices) - set(test_indices))\nrandom.seed()\nrandom.shuffle(remaining_indices)\n\ntrain_indices = remaining_indices[:num_train]\nval_indices = remaining_indices[num_train:num_train + num_val]\n\ntrain_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\nval_sampler = torch.utils.data.SubsetRandomSampler(val_indices)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We are making sure we have no common indicies from the training validation and test set ","metadata":{}},{"cell_type":"code","source":"set_train_indices = set(train_indices)\nset_val_indices = set(val_indices)\nset_test_indices = set(test_indices)\n\ncommon_indices_test = set_train_indices.intersection(set_test_indices)\ncommon_indices_val = set_train_indices.intersection(set_val_indices)\n\nprint(f\"Number of common indices in train-test: {len(common_indices_test)}\")\nprint(f\"Number of common indices in train-val: {len(common_indices_val)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.676854Z","iopub.execute_input":"2023-12-09T17:49:25.677181Z","iopub.status.idle":"2023-12-09T17:49:25.691464Z","shell.execute_reply.started":"2023-12-09T17:49:25.677154Z","shell.execute_reply":"2023-12-09T17:49:25.690452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining Our Custom Baseline Model","metadata":{}},{"cell_type":"code","source":"class OurCustomModel(nn.Module):\n    def __init__(self, num_features, num_classes, conv_w = [64,128,256,512], droupout_rate = 0.1):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(1, conv_w[0], kernel_size=3, stride=1, padding=1) #1 in channel input for 1 channel image\n        self.conv2 = nn.Conv2d(conv_w[0], conv_w[1], kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(conv_w[1], conv_w[2], kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(conv_w[2], conv_w[3], kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n\n        self.fc1 = nn.Linear(conv_w[3], 512) # based on the 4th conv shape\n        self.fc2 = nn.Linear(512, num_classes)\n\n        self.dropout = nn.Dropout(droupout_rate)\n        \n    def forward(self, x): \n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.dropout(x)\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout(x)\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.dropout(x)\n        x = self.pool(F.relu(self.conv4(x)))\n        x = self.dropout(x)\n        num_features = x.size(1) * x.size(2) * x.size(3)\n        x = x.view(-1, num_features)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.692594Z","iopub.execute_input":"2023-12-09T17:49:25.692873Z","iopub.status.idle":"2023-12-09T17:49:25.706832Z","shell.execute_reply.started":"2023-12-09T17:49:25.692848Z","shell.execute_reply":"2023-12-09T17:49:25.705926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining Our Audio Data Classifier for the traininng and validation","metadata":{}},{"cell_type":"code","source":"class AudioClassifier(pl.LightningModule):\n    def __init__(self, model, class_weights, learning_rate = 0.001, weight_decay = 0.01, optimizer= 'adamw'):\n        super(AudioClassifier, self).__init__()    \n        self.model = model\n        self.lr = learning_rate\n        self.w_dec = weight_decay\n        self.optimizer = optimizer\n        self.class_weights = class_weights\n\n    def forward(self, x): \n        return self.model(x)\n     \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.squeeze()\n        y_pred = self(x)\n        train_loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n        y_pred = torch.argmax(self(x), dim=1)\n        train_acc = torch.sum(y_pred == y).item() / y.size(0)\n        self.log('train_loss', train_loss.item(), on_epoch=True, on_step=True)\n        self.log('train_acc', train_acc, on_epoch=True, on_step=True)\n        print(f\"Batch: {batch_idx}\")\n        print(f'Train loss: {train_loss.item()}')\n        print(f'Train acc: {train_acc: .5f}')\n        return train_loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.squeeze()\n        y_pred = self(x)\n        val_loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n        y_pred = torch.argmax(self(x), dim=1)\n        val_acc = torch.sum(y_pred == y).item() / y.size(0)\n        self.log('val_loss', val_loss.item(), on_epoch=True)\n        self.log('val_acc', val_acc, on_epoch=True, )\n        print(f\"Batch: {batch_idx}\")\n        print(f'Val loss: {val_loss.item()}')\n        print(f'Val acc: {val_acc: .5f}')\n        return val_loss\n        \n    def configure_optimizers(self):\n        if self.optimizer == \"adamw\":\n            optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.w_dec)\n        elif self.optimizer == \"sgd\":\n            optimizer = optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.w_dec)\n        elif self.optimizer == \"rmsprop\":\n            optimizer = optim.RMSprop(self.parameters(), lr=self.lr, weight_decay=self.w_dec)\n        else:\n            raise ValueError(f\"Unsupported optimizer\")\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.709966Z","iopub.execute_input":"2023-12-09T17:49:25.710258Z","iopub.status.idle":"2023-12-09T17:49:25.724523Z","shell.execute_reply.started":"2023-12-09T17:49:25.710232Z","shell.execute_reply":"2023-12-09T17:49:25.723721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Phase","metadata":{}},{"cell_type":"markdown","source":"#### Defining our training method where we configure WanDB logging, initializing the model, the classifier and starting the training with the previous configurations","metadata":{}},{"cell_type":"code","source":"def train():\n    wandb.finish()\n    run = wandb.init()\n    \n    config = wandb.config\n\n    train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=64, sampler=val_sampler, shuffle=False, num_workers=4)\n   \n    num_classes = 264\n    class_weights = train_dataset.class_weights.cuda() if torch.cuda.is_available() else train_dataset.class_weights\n  \n    model_name = 'model-pludzsln:v14'\n    artifact = run.use_artifact('deepbirding/deepbirding/' + model_name, type='model')\n    artifact_dir = artifact.download()\n    checkpoint = torch.load(f'/kaggle/working/artifacts/{model_name}/model.ckpt')\n\n    model = torchvision.models.resnet34(pretrained=True)\n    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    \n    model_dict = model.state_dict()\n    for key in model_dict.keys():\n        if key in checkpoint.keys() and model_dict[key].shape == checkpoint[key].shape:\n            model_dict[key] = checkpoint[key]\n    model.load_state_dict(model_dict)\n    \n    lit_model = AudioClassifier(\n        model = model,\n        class_weights = class_weights,\n        learning_rate=config.lr,\n        optimizer=config.optimizer)\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        filename='best_model',\n        verbose=True,\n        save_weights_only=True,\n    )\n    early_stopping_callback = pl.callbacks.EarlyStopping(monitor=\"val_acc\", patience=3, verbose=True, mode=\"max\")\n    \n    wandb_logger = WandbLogger(project=project_name, log_model='all')\n    wandb_logger.watch(model,log_graph=False)\n\n    trainer = pl.Trainer(accelerator='gpu',\n                         max_epochs=50,\n                         devices=1, \n                         precision=\"16-mixed\",\n                         logger=wandb_logger,\n                         callbacks=[checkpoint_callback, early_stopping_callback]\n                        )\n\n    trainer.fit(lit_model,train_loader,val_loader)\n    wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.738254Z","iopub.execute_input":"2023-12-09T17:49:25.738567Z","iopub.status.idle":"2023-12-09T17:49:25.751488Z","shell.execute_reply.started":"2023-12-09T17:49:25.738531Z","shell.execute_reply":"2023-12-09T17:49:25.750624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We are using automated hyperparameter optimization for our training, but we are also manually correcting the value ranges according to the conclusions of the previous trainings, the original values can be found in the commented sections","metadata":{}},{"cell_type":"code","source":"wandb.login()\nproject_name = 'deepbirding'\nPARAM_OPT = False\nif PARAM_OPT:\n    sweep_config = {\n        'method': 'bayes',\n        'name': 'sweep',\n        'metric': {\n            'goal': 'maximize', \n            'name': 'val_acc',\n            },\n        'parameters': {\n            'lr': {'max': 8e-4, 'min': 7e-4},\n            'optimizer': {'values': ['adamw']}, #'sgd', 'rmsprop']},\n            'batch_size': {'values': [64]},#16, 32, 64]},\n            'weight_decay': {'max': 2e-3, 'min': 1e-3},#-5},\n            'conv_w': {'values': [[64,128,256,512]]}#[16,32,64,128],[32,64,128,256]]}\n        }\n    }\n    sweep_id = wandb.sweep(sweep=sweep_config, project=project_name)\n    wandb.agent(sweep_id=sweep_id, function=train)\nelse:\n    train()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:49:25.752538Z","iopub.execute_input":"2023-12-09T17:49:25.752824Z","iopub.status.idle":"2023-12-09T17:51:56.591660Z","shell.execute_reply.started":"2023-12-09T17:49:25.752799Z","shell.execute_reply":"2023-12-09T17:51:56.590602Z"},"trusted":true},"execution_count":null,"outputs":[]}]}