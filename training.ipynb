{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 44224,
     "databundleVersionId": 5188730,
     "sourceType": "competition"
    },
    {
     "sourceId": 6912431,
     "sourceType": "datasetVersion",
     "datasetId": 3969818
    },
    {
     "sourceId": 7137902,
     "sourceType": "datasetVersion",
     "datasetId": 4119162
    }
   ],
   "dockerImageVersionId": 30558,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install audiomentations -q",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-12-10T09:36:28.803574700Z",
     "start_time": "2023-12-10T09:36:17.228464200Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Importing the essential packages for the training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import random\n",
    "import wandb\n",
    "import torch.utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pickle\n",
    "from audiomentations import Compose, AddGaussianNoise, Gain, AddBackgroundNoise, PitchShift, Shift, TimeMask, AdjustDuration, Normalize\n",
    "import torchvision"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:11.433859Z",
     "iopub.execute_input": "2023-12-09T17:49:11.434692Z",
     "iopub.status.idle": "2023-12-09T17:49:25.495130Z",
     "shell.execute_reply.started": "2023-12-09T17:49:11.434652Z",
     "shell.execute_reply": "2023-12-09T17:49:25.494056Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-12-10T09:36:45.878594500Z",
     "start_time": "2023-12-10T09:36:41.714091600Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlibrosa\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwandb\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'wandb'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Defining some functions for loading the dataset, such as normalizing the Mel Spectogram, extracting the metadata, for the imbalanced dataset we are using class weighting, and we are also endocing our labels\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize_mel_spectrogram(mel_spec_tensor):\n",
    "    min_value = mel_spec_tensor.min()\n",
    "    max_value = mel_spec_tensor.max()\n",
    "    normalized_mel_spec = (mel_spec_tensor - min_value) / (max_value - min_value)\n",
    "    return normalized_mel_spec\n",
    "\n",
    "def extract_metadata(metadata_file):\n",
    "    metadata_df = pd.read_csv(metadata_file)\n",
    "    metadata_df['Filename'] = metadata_df['Filename'].apply(lambda x: x.split(\"/\")[-1])\n",
    "    metadata = metadata_df[['Label', 'Filename']].reset_index(drop=True)\n",
    "    metadata.columns = ['label', 'filename']\n",
    "    return metadata\n",
    "\n",
    "def class_weights(labels):\n",
    "    class_weights = (labels.value_counts() / labels.value_counts().sum()) ** (-0.5)\n",
    "    normalized_weights = class_weights / class_weights.sum()\n",
    "    normalized_weights = normalized_weights*80\n",
    "    return torch.tensor(normalized_weights.values, dtype=torch.float16)\n",
    "\n",
    "def encode_labels(labels):\n",
    "    unique_labels = labels.unique()\n",
    "    encoded_dict = {label: num for label, num in zip(unique_labels, range(len(unique_labels)))}\n",
    "    return encoded_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Defining Our Custom Audio Classification Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AudioClassificationDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata_file, apply_augmentations=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = extract_metadata(metadata_file)\n",
    "        self.apply_augmentations = apply_augmentations\n",
    "        self.label_encoder = encode_labels(self.metadata['label']) \n",
    "        self.class_weights = class_weights(self.metadata['label'])\n",
    "        self.augmentations = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.4),\n",
    "            Gain(min_gain_in_db=-12, max_gain_in_db=12, p=0.4),    \n",
    "            Shift(min_shift=1, max_shift=6, shift_unit=\"seconds\", p=0.6),\n",
    "            AddBackgroundNoise(sounds_path=[\"/kaggle/input/backg-noise/rain-noise.wav\"],max_absolute_rms_db=-30, p=0.3),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.metadata['label'][idx]\n",
    "        filename = self.metadata['filename'][idx]\n",
    "        audio_file = os.path.join(self.data_dir, label, filename) \n",
    "        try:\n",
    "            waveform, sample_rate = librosa.load(audio_file, sr=None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {audio_file}: {e}\")\n",
    "            return None   \n",
    "        if self.apply_augmentations:\n",
    "            waveform = self.augmentations(waveform, sample_rate=sample_rate)\n",
    "        normalization_transform = Normalize(p=1.0)\n",
    "        duration_transform = AdjustDuration(duration_seconds=10,padding_mode=\"wrap\", p=1.0)\n",
    "        waveform = normalization_transform(waveform, sample_rate=sample_rate)\n",
    "        waveform = duration_transform(waveform, sample_rate=sample_rate)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate, fmin=800, fmax=12000)\n",
    "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        mel_spectrogram_db = normalize_mel_spectrogram(mel_spectrogram_db)\n",
    "        mel_spec_db_tensor = torch.from_numpy(mel_spectrogram_db).unsqueeze(0)\n",
    "        class_label = self.label_encoder[label]\n",
    "        class_label_tensor = torch.tensor(class_label).unsqueeze(0)\n",
    "        return mel_spec_db_tensor, class_label_tensor"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.517400Z",
     "iopub.execute_input": "2023-12-09T17:49:25.517721Z",
     "iopub.status.idle": "2023-12-09T17:49:25.538548Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.517693Z",
     "shell.execute_reply": "2023-12-09T17:49:25.537562Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating two different datasets, one with augmentations applied for the training, one without augmentations for the validation part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir = '/kaggle/input/birdclef-2023/train_audio'\n",
    "metadata_file = '/kaggle/input/filtered-metadata/filtered_metadata.csv'\n",
    "train_dataset = AudioClassificationDataset(data_dir, metadata_file, apply_augmentations=True)\n",
    "val_dataset = AudioClassificationDataset(data_dir, metadata_file, apply_augmentations=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.539741Z",
     "iopub.execute_input": "2023-12-09T17:49:25.540483Z",
     "iopub.status.idle": "2023-12-09T17:49:25.640587Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.540456Z",
     "shell.execute_reply": "2023-12-09T17:49:25.639781Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### We split the original dataset with 70% training 20% validation and 10% test parts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1  \n",
    "\n",
    "num_samples = len(train_dataset)\n",
    "num_train = int(train_ratio * num_samples)\n",
    "num_val = int(val_ratio * num_samples)\n",
    "num_test = int(test_ratio * num_samples)\n",
    "print(f'Training indices: {num_train}  Validation indices: {num_val} Test indicies: {num_test}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### We are making sure that the test indicies are selected randomly but most importantly deterministically with 42 seed, so we can have uniform indicies for testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "indices = list(range(num_samples))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "test_indices = indices[-num_test:]\n",
    "with open('test_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(test_indices, f)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.641993Z",
     "iopub.execute_input": "2023-12-09T17:49:25.642358Z",
     "iopub.status.idle": "2023-12-09T17:49:25.675510Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.642323Z",
     "shell.execute_reply": "2023-12-09T17:49:25.674560Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The reamining indicies can be chose for the training and validation indicies "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "remaining_indices = list(set(indices) - set(test_indices))\n",
    "random.seed()\n",
    "random.shuffle(remaining_indices)\n",
    "\n",
    "train_indices = remaining_indices[:num_train]\n",
    "val_indices = remaining_indices[num_train:num_train + num_val]\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### We are making sure we have no common indicies from the training validation and test set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "set_train_indices = set(train_indices)\nset_val_indices = set(val_indices)\nset_test_indices = set(test_indices)\n\ncommon_indices_test = set_train_indices.intersection(set_test_indices)\ncommon_indices_val = set_train_indices.intersection(set_val_indices)\n\nprint(f\"Number of common indices in train-test: {len(common_indices_test)}\")\nprint(f\"Number of common indices in train-val: {len(common_indices_val)}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.676854Z",
     "iopub.execute_input": "2023-12-09T17:49:25.677181Z",
     "iopub.status.idle": "2023-12-09T17:49:25.691464Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.677154Z",
     "shell.execute_reply": "2023-12-09T17:49:25.690452Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining Our Custom Baseline Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class OurCustomModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, conv_w = [64,128,256,512], droupout_rate = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, conv_w[0], kernel_size=3, stride=1, padding=1) #1 in channel input for 1 channel image\n",
    "        self.conv2 = nn.Conv2d(conv_w[0], conv_w[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(conv_w[1], conv_w[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(conv_w[2], conv_w[3], kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_w[3], 512) # based on the 4th conv shape\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(droupout_rate)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        num_features = x.size(1) * x.size(2) * x.size(3)\n",
    "        x = x.view(-1, num_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.692594Z",
     "iopub.execute_input": "2023-12-09T17:49:25.692873Z",
     "iopub.status.idle": "2023-12-09T17:49:25.706832Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.692848Z",
     "shell.execute_reply": "2023-12-09T17:49:25.705926Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining Our Audio Data Classifier for the traininng and validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AudioClassifier(pl.LightningModule):\n",
    "    def __init__(self, model, class_weights, learning_rate = 0.001, weight_decay = 0.01, optimizer= 'adamw'):\n",
    "        super(AudioClassifier, self).__init__()    \n",
    "        self.model = model\n",
    "        self.lr = learning_rate\n",
    "        self.w_dec = weight_decay\n",
    "        self.optimizer = optimizer\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.model(x)\n",
    "     \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.squeeze()\n",
    "        y_pred = self(x)\n",
    "        train_loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n",
    "        y_pred = torch.argmax(self(x), dim=1)\n",
    "        train_acc = torch.sum(y_pred == y).item() / y.size(0)\n",
    "        self.log('train_loss', train_loss.item(), on_epoch=True, on_step=True)\n",
    "        self.log('train_acc', train_acc, on_epoch=True, on_step=True)\n",
    "        print(f\"Batch: {batch_idx}\")\n",
    "        print(f'Train loss: {train_loss.item()}')\n",
    "        print(f'Train acc: {train_acc: .5f}')\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.squeeze()\n",
    "        y_pred = self(x)\n",
    "        val_loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n",
    "        y_pred = torch.argmax(self(x), dim=1)\n",
    "        val_acc = torch.sum(y_pred == y).item() / y.size(0)\n",
    "        self.log('val_loss', val_loss.item(), on_epoch=True)\n",
    "        self.log('val_acc', val_acc, on_epoch=True, )\n",
    "        print(f\"Batch: {batch_idx}\")\n",
    "        print(f'Val loss: {val_loss.item()}')\n",
    "        print(f'Val acc: {val_acc: .5f}')\n",
    "        return val_loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.w_dec)\n",
    "        elif self.optimizer == \"sgd\":\n",
    "            optimizer = optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.w_dec)\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            optimizer = optim.RMSprop(self.parameters(), lr=self.lr, weight_decay=self.w_dec)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer\")\n",
    "        return optimizer"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.709966Z",
     "iopub.execute_input": "2023-12-09T17:49:25.710258Z",
     "iopub.status.idle": "2023-12-09T17:49:25.724523Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.710232Z",
     "shell.execute_reply": "2023-12-09T17:49:25.723721Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training Phase",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining our training method where we configure wand logging, the model, the classidier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    wandb.finish()\n",
    "    run = wandb.init()\n",
    "    \n",
    "    config = wandb.config\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, sampler=val_sampler, shuffle=False, num_workers=4)\n",
    "   \n",
    "    num_classes = 264\n",
    "    class_weights = train_dataset.class_weights.cuda() if torch.cuda.is_available() else train_dataset.class_weights\n",
    "  \n",
    "    model_name = 'model-pludzsln:v14'\n",
    "    artifact = run.use_artifact('deepbirding/deepbirding/' + model_name, type='model')\n",
    "    artifact_dir = artifact.download()\n",
    "    checkpoint = torch.load(f'/kaggle/working/artifacts/{model_name}/model.ckpt')\n",
    "\n",
    "    model = torchvision.models.resnet34(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    model_dict = model.state_dict()\n",
    "    for key in model_dict.keys():\n",
    "        if key in checkpoint.keys() and model_dict[key].shape == checkpoint[key].shape:\n",
    "            model_dict[key] = checkpoint[key]\n",
    "    model.load_state_dict(model_dict)\n",
    "    \n",
    "    lit_model = AudioClassifier(\n",
    "        model = model,\n",
    "        class_weights = class_weights,\n",
    "        conv_w = config.conv_w,\n",
    "        droupout_rate=config.dropout,\n",
    "        learning_rate=config.lr,\n",
    "        optimizer=config.optimizer,\n",
    "        weight_decay=config.weight_decay)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_acc',\n",
    "        mode='max',\n",
    "        filename='best_model',\n",
    "        verbose=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    early_stopping_callback = pl.callbacks.EarlyStopping(monitor=\"val_acc\", patience=3, verbose=True, mode=\"max\")\n",
    "    \n",
    "    wandb_logger = WandbLogger(project=project_name, log_model='all')\n",
    "    wandb_logger.watch(model,log_graph=False)\n",
    "\n",
    "    trainer = pl.Trainer(accelerator='gpu',\n",
    "                         max_epochs=50,\n",
    "                         devices=1, \n",
    "                         precision=\"16-mixed\",\n",
    "                         logger=wandb_logger,\n",
    "                         callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "                        )\n",
    "\n",
    "    trainer.fit(lit_model,train_loader,val_loader)\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.738254Z",
     "iopub.execute_input": "2023-12-09T17:49:25.738567Z",
     "iopub.status.idle": "2023-12-09T17:49:25.751488Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.738531Z",
     "shell.execute_reply": "2023-12-09T17:49:25.750624Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "wandb.login()\nproject_name = 'deepbirding'\nPARAM_OPT = False\nif PARAM_OPT:\n    sweep_config = {\n        'method': 'bayes',\n        'name': 'sweep',\n        'metric': {\n            'goal': 'maximize', \n            'name': 'val_acc',\n            },\n        'parameters': {\n            'lr': {'max': 8e-4, 'min': 7e-4},\n            'optimizer': {'values': ['adamw']}, #'sgd', 'rmsprop']},\n            'dropout': {'values': [0]},#, 0.3, 0.4] },\n            'batch_size': {'values': [64]},#16, 32, 64]},\n            'weight_decay': {'max': 2e-3, 'min': 1e-3},#-5},\n            'conv_w': {'values': [[64,128,256,512]]}#[16,32,64,128],[32,64,128,256]]}\n        }\n    }\n    sweep_id = wandb.sweep(sweep=sweep_config, project=project_name)\n    wandb.agent(sweep_id=sweep_id, function=train)\nelse:\n    train()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-09T17:49:25.752538Z",
     "iopub.execute_input": "2023-12-09T17:49:25.752824Z",
     "iopub.status.idle": "2023-12-09T17:51:56.591660Z",
     "shell.execute_reply.started": "2023-12-09T17:49:25.752799Z",
     "shell.execute_reply": "2023-12-09T17:51:56.590602Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
